{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valeriy1990/Neural_networks/blob/main/%D0%A2%D0%B5%D0%BB%D0%B5%D0%B3%D1%80%D0%B0%D0%BC-%D0%B1%D0%BE%D1%82%2C%20%D0%BE%D1%82%D0%B2%D0%B5%D1%87%D0%B0%D1%8E%D1%89%D0%B8%D0%B9%20%D0%BD%D0%B0%20%D0%B2%D0%BE%D0%BF%D1%80%D0%BE%D1%81%D1%8B%20%D0%B8%D0%B7%20%D0%B1%D0%B0%D0%B7%D1%8B%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85%20(%D0%BF%D0%B0%D1%80%D1%81%D0%B8%D0%BD%D0%B3%20%D0%92%D0%B8%D0%BA%D0%B8%D0%BF%D0%B5%D0%B4%D0%B8%D0%B8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–¢–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç, –∫–æ—Ç–æ—Ä—ã–π –±—É–¥–µ—Ç –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ –≤–∞—à–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c6WZSsChNoIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "SbG41nTn7O0R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api aiogram asyncio nest_asyncio openai tiktoken"
      ],
      "metadata": {
        "id": "ICTl6UdRPHBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716ae699-16f4-4449-dc6f-ff66798033ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: aiogram in /usr/local/lib/python3.12/dist-packages (3.22.0)\n",
            "Requirement already satisfied: asyncio in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.107.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from wikipedia-api) (2.32.4)\n",
            "Requirement already satisfied: aiofiles<24.2,>=23.2.1 in /usr/local/lib/python3.12/dist-packages (from aiogram) (24.1.0)\n",
            "Requirement already satisfied: aiohttp<3.13,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from aiogram) (3.12.15)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.12/dist-packages (from aiogram) (2025.8.3)\n",
            "Requirement already satisfied: magic-filter<1.1,>=1.0.12 in /usr/local/lib/python3.12/dist-packages (from aiogram) (1.0.12)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from aiogram) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions<=5.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from aiogram) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.4.1->aiogram) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.4.1->aiogram) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.4.1->aiogram) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "import logging\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.filters import Command\n",
        "import tiktoken\n",
        "import os\n",
        "import getpass\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from scipy import spatial  # –≤—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–¥—Å—Ç–≤–æ –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re  # –¥–ª—è –≤—ã—Ä–µ–∑–∞–Ω–∏—è —Å—Å—ã–ª–æ–∫ <ref> –∏–∑ —Å—Ç–∞—Ç–µ–π –í–∏–∫–∏–ø–µ–¥–∏–∏\n",
        "import time\n",
        "from google.colab import files\n",
        "import random\n",
        "import wikipediaapi\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import json\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "nest_asyncio.apply()\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "sJ36fUZpMlPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81401d48-0d74-435f-b9dd-07cbb6349d75"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–ª—é—á–∞ API –æ—Ç VseGpt\n",
        "key = userdata.get('VSEGPT_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = key\n",
        "\n",
        "# –ê–¥—Ä–µ—Å —Å–µ—Ä–≤–µ—Ä–∞ VseGpt\n",
        "base_url = \"https://api.vsegpt.ru/v1\"\n",
        "os.environ[\"OPENAI_BASE_URL\"] = base_url\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å GPT\n",
        "# GPT_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "  api_key=key,\n",
        "  base_url=\"https://api.vsegpt.ru/v1\"\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RSETnR-qPo5-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–∏—Å–∫–∞"
      ],
      "metadata": {
        "id": "EPGHztnK34nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –ó–∞–º–µ–Ω–∏—Ç–µ '–í–ê–®–ê_–ú–û–î–ï–õ–¨_–î–õ–Ø_–¢–û–ö–ï–ù–ò–ó–ê–¶–ò–ò' –Ω–∞ –∏–º—è –º–æ–¥–µ–ª–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏ OpenAI\n",
        "# –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–ª—è –º–æ–¥–µ–ª–µ–π gpt-3.5-turbo –∏–ª–∏ gpt-4 –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ \"cl100k_base\"\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "    try:\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        response = client.embeddings.create(\n",
        "            input=[text],\n",
        "            model=model\n",
        "        )\n",
        "        return response.data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}\")\n",
        "        return None\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    \"\"\"–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞.\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return len(encoding.encode(text))"
      ],
      "metadata": {
        "id": "An3IPTkX1LFa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    language='en',\n",
        "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
        "    user_agent='MyWikipediaBot/1.0'\n",
        ")\n",
        "\n",
        "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
        "CATEGORY_NAME = \"Category:Psychology\"\n",
        "IGNORED_SECTIONS = [\n",
        "    \"See also\",\n",
        "    \"References\",\n",
        "    \"External links\",\n",
        "    \"Further reading\",\n",
        "    \"Footnotes\",\n",
        "    \"Bibliography\",\n",
        "    \"Sources\",\n",
        "    \"Citations\",\n",
        "    \"Literature\",\n",
        "    \"Footnotes\",\n",
        "    \"Notes and references\",\n",
        "    \"Photo gallery\",\n",
        "    \"Works cited\",\n",
        "    \"Photos\",\n",
        "    \"Gallery\",\n",
        "    \"Notes\",\n",
        "    \"References and sources\",\n",
        "    \"References and notes\",\n",
        "]\n",
        "MAX_TOKENS = 4000"
      ],
      "metadata": {
        "id": "2_OQpupp1fCD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_chunks(text, max_tokens=MAX_TOKENS):\n",
        "    def count_tokens(text):\n",
        "        return len(text) // 4\n",
        "\n",
        "    if count_tokens(text) <= max_tokens:\n",
        "        return [text]\n",
        "\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    if len(paragraphs) > 1:\n",
        "        chunks = []\n",
        "        for paragraph in paragraphs:\n",
        "            if paragraph.strip():\n",
        "                chunks.extend(split_into_chunks(paragraph, max_tokens))\n",
        "        return chunks\n",
        "\n",
        "    sentences = sent_tokenize(text, language='english')\n",
        "    if len(sentences) > 1:\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_token_count = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = count_tokens(sentence)\n",
        "\n",
        "            if current_token_count + sentence_tokens > max_tokens and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_token_count = sentence_tokens\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_token_count += sentence_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    mid = len(text) // 2\n",
        "    split_pos = text.rfind(' ', 0, mid) or text.find(' ', mid)\n",
        "    if split_pos == -1:\n",
        "        split_pos = mid\n",
        "\n",
        "    part1 = text[:split_pos]\n",
        "    part2 = text[split_pos:]\n",
        "\n",
        "    return split_into_chunks(part1, max_tokens) + split_into_chunks(part2, max_tokens)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def process_article(title):\n",
        "    page = wiki_wiki.page(title)\n",
        "    if not page.exists():\n",
        "        return []\n",
        "\n",
        "    sections = []\n",
        "    for section in page.sections:\n",
        "        if section.title in IGNORED_SECTIONS:\n",
        "            continue\n",
        "\n",
        "        section_text = f\"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {section.title}\\n{clean_text(section.text)}\"\n",
        "        section_chunks = split_into_chunks(section_text)\n",
        "        sections.extend(section_chunks)\n",
        "\n",
        "    return sections\n",
        "\n",
        "def get_category_members(category, depth=1):\n",
        "    if depth == 0:\n",
        "        return []\n",
        "\n",
        "    pages = []\n",
        "    for member in category.categorymembers.values():\n",
        "        if member.ns == wikipediaapi.Namespace.CATEGORY and depth > 1:\n",
        "            pages.extend(get_category_members(member, depth-1))\n",
        "        elif member.ns == wikipediaapi.Namespace.MAIN:\n",
        "            pages.append(member)\n",
        "\n",
        "    return pages"
      ],
      "metadata": {
        "id": "j0SEheln1u2e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏...\")\n",
        "category = wiki_wiki.page(CATEGORY_NAME)\n",
        "if not category.exists():\n",
        "    print(f\"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {CATEGORY_NAME} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!\")\n",
        "else:\n",
        "    articles = get_category_members(category)\n",
        "    knowledge_base = []\n",
        "\n",
        "    for article in articles:\n",
        "        print(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: {article.title}\")\n",
        "        fragments = process_article(article.title)  # —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫\n",
        "\n",
        "        for fragment in fragments:\n",
        "            try:\n",
        "                embedding = get_embedding(fragment)\n",
        "                knowledge_base.append({\n",
        "                    \"text\": fragment,\n",
        "                    \"embedding\": embedding,\n",
        "                    \"source\": article.title  # –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –æ—Ç–∫—É–¥–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç\n",
        "                })\n",
        "                print(f\"  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}\")\n",
        "\n",
        "    print(f\"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç {len(knowledge_base)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRG2AAzL1y2d",
        "outputId": "3499e9d1-577c-4dea-e48b-9620e59c1f4e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏...\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Psychology\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Outline of psychology\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Alarmism\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Aphantasia\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Belief congruence\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Binocular disparity\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Binocular vision\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Biology of romantic love\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Black fatigue\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Cognitive immunization\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Counterphobic attitude\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Cybersexuality\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Double-nail illusion\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Folk economics\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Fragile masculinity\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Frequency illusion\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Impact of the COVID-19 pandemic on time perception\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Limerence\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Love addiction\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Musical escapism\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Obsessive love\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Passionate and companionate love\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Perceptual vigilance\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Psychiatry\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Psychologist\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Rapture anxiety\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Resilience week\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Social buffering\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏: Ultra-realism\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "  ‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω\n",
            "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ–¥–µ—Ä–∂–∏—Ç 125 —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "with open(\"knowledge_base.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(knowledge_base, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ knowledge_base.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-kQMqPL2KJr",
        "outputId": "84fce60d-f78b-4b69-9dec-5740391ef4b1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ knowledge_base.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–∞–º–∏.\"\"\"\n",
        "    return np.dot(a, b) / (norm(a) * norm(b))"
      ],
      "metadata": {
        "id": "b-EOZ73Y2caE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ Telegram-–±–æ—Ç–∞\n",
        "API_TOKEN = userdata.get('API_TOKEN')\n",
        "bot = Bot(token=API_TOKEN)\n",
        "dp = Dispatcher()\n",
        "\n",
        "# –ö–æ–º–∞–Ω–¥–∞ /help\n",
        "@dp.message(Command(\"help\"))\n",
        "async def send_help(message: types.Message):\n",
        "    help_text = (\n",
        "        \"–¢–µ–º–∞—Ç–∏–∫–∞: –ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ –í–∏–∫–∏–ø–µ–¥–∏–∏\\n\"\n",
        "        f\"–ß–∏—Å–ª–æ –∑–∞–ø–∏—Å–µ–π: {len(knowledge_base)}\\n\"\n",
        "        \"–ü—Ä–∏–º–µ—Ä –∑–∞–ø—Ä–æ—Å–∞: 'What is Alarmism?'\"\n",
        "    )\n",
        "    await message.answer(help_text)\n",
        "\n",
        "@dp.message()\n",
        "async def handle_query(message: types.Message):\n",
        "    query = message.text\n",
        "    user_id = message.from_user.id\n",
        "\n",
        "    # 1. –°—á–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∑–∞–ø—Ä–æ—Å–∞\n",
        "    query_tokens = count_tokens(query)\n",
        "    if query_tokens > 4000:\n",
        "        await message.answer(\"–í–∞—à –∑–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π (–±–æ–ª–µ–µ 4000 —Ç–æ–∫–µ–Ω–æ–≤).\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 2. –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞\n",
        "        query_embedding = get_embedding(query)\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}\")\n",
        "        await message.answer(\"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤–∞—à –∑–∞–ø—Ä–æ—Å.\")\n",
        "        return\n",
        "\n",
        "    # 3. –°—á–∏—Ç–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–ª—è –≤—Å–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤\n",
        "    scored_chunks = []\n",
        "    for chunk in knowledge_base:\n",
        "        similarity = cosine_similarity(query_embedding, chunk[\"embedding\"])\n",
        "        text_tokens = count_tokens(chunk[\"text\"])\n",
        "        scored_chunks.append({\n",
        "            \"text\": chunk[\"text\"],\n",
        "            \"source\": chunk.get(\"source\", \"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ\"),\n",
        "            \"similarity\": similarity,\n",
        "            \"tokens\": text_tokens\n",
        "        })\n",
        "\n",
        "    # 4. –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (–æ—Ç —Å–∞–º–æ–≥–æ –ø–æ—Ö–æ–∂–µ–≥–æ –∫ —Å–∞–º–æ–º—É –Ω–µ–ø–æ—Ö–æ–∂–µ–º—É)\n",
        "    scored_chunks.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
        "\n",
        "    # 5. –ù–∞–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç, –ø–æ–∫–∞ –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º 4000 —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    relevant_chunks = []\n",
        "    current_tokens = query_tokens  # –Ω–∞—á–∏–Ω–∞–µ–º —Å —Ç–æ–∫–µ–Ω–æ–≤ –≤–æ–ø—Ä–æ—Å–∞\n",
        "\n",
        "    # –î–æ–±–∞–≤–∏–º —Ç–æ–∫–µ–Ω—ã —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ ‚Äî —ç—Ç–æ –≤–∞–∂–Ω–æ!\n",
        "    system_prompt = \"You are a helpful assistant that answers questions based on the provided context.\"\n",
        "    system_tokens = count_tokens(system_prompt)\n",
        "    current_tokens += system_tokens\n",
        "\n",
        "    # –¢–∞–∫–∂–µ —É—á—Ç—ë–º —Ç–æ–∫–µ–Ω—ã –Ω–∞ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–∑–∞–≥–æ–ª–æ–≤–∫–∏, —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –∏ —Ç.–ø.)\n",
        "    # –ü—Ä–∏–º–µ—Ä–Ω–æ: \"–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å... \\n\\n–í–æ–ø—Ä–æ—Å: ...\" ‚Äî –¥–æ–±–∞–≤–∏–º –∑–∞–ø–∞—Å 50 —Ç–æ–∫–µ–Ω–æ–≤\n",
        "    overhead_tokens = 50\n",
        "    current_tokens += overhead_tokens\n",
        "\n",
        "    # –¢–µ–ø–µ—Ä—å –¥–æ–±–∞–≤–ª—è–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã, –ø–æ–∫–∞ –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏–º–∏—Ç\n",
        "    for chunk in scored_chunks:\n",
        "        if current_tokens + chunk[\"tokens\"] <= 4000:\n",
        "            relevant_chunks.append(chunk)\n",
        "            current_tokens += chunk[\"tokens\"]\n",
        "        else:\n",
        "            break  # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º, –µ—Å–ª–∏ —Å–ª–µ–¥—É—é—â–∏–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç –≤—ã–≤–µ–¥–µ—Ç –∑–∞ –ª–∏–º–∏—Ç\n",
        "\n",
        "    if not relevant_chunks:\n",
        "        await message.answer(\"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤.\")\n",
        "        return\n",
        "\n",
        "    # 6. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
        "    context = (\n",
        "        \"–û—Ç–≤–µ—Ç—å –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. \"\n",
        "        \"–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Å–∫–∞–∂–∏, —á—Ç–æ –Ω–µ –º–æ–∂–µ—à—å –æ—Ç–≤–µ—Ç–∏—Ç—å.\\n\\n\"\n",
        "    )\n",
        "    for chunk in relevant_chunks:\n",
        "        context += chunk[\"text\"] + \"\\n\\n\"\n",
        "    context += f\"–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}\"\n",
        "\n",
        "    # 7. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ –º–æ–¥–µ–ª—å\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            # model=\"deepseek-chat-v3-0324:free\",\n",
        "            model=\"openai/gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": context}\n",
        "            ]\n",
        "        )\n",
        "        answer = response.choices[0].message.content\n",
        "\n",
        "        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏\n",
        "        sources = set(chunk[\"source\"] for chunk in relevant_chunks)\n",
        "        if sources:\n",
        "            answer += \"\\n\\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: \" + \", \".join(sources)\n",
        "\n",
        "        await message.answer(answer)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ API: {e}\")\n",
        "        await message.answer(\"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞.\")\n"
      ],
      "metadata": {
        "id": "fQvjPHh-2flB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞\n",
        "async def main():\n",
        "    print(\"–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω\")\n",
        "    await dp.start_polling(bot)\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞\n",
        "print(\"–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...\")\n",
        "try:\n",
        "    asyncio.run(main())\n",
        "except Exception as e:\n",
        "    print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV94_yK-1LQR",
        "outputId": "c20e178b-e202-457e-ffcb-bbce8ed7744c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...\n",
            "–ë–æ—Ç –∑–∞–ø—É—â–µ–Ω\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:aiogram.dispatcher:Received SIGINT signal\n"
          ]
        }
      ]
    }
  ]
}