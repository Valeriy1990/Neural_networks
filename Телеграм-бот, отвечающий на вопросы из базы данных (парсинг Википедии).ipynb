{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Valeriy1990/Neural_networks/blob/main/%D0%A2%D0%B5%D0%BB%D0%B5%D0%B3%D1%80%D0%B0%D0%BC-%D0%B1%D0%BE%D1%82%2C%20%D0%BE%D1%82%D0%B2%D0%B5%D1%87%D0%B0%D1%8E%D1%89%D0%B8%D0%B9%20%D0%BD%D0%B0%20%D0%B2%D0%BE%D0%BF%D1%80%D0%BE%D1%81%D1%8B%20%D0%B8%D0%B7%20%D0%B1%D0%B0%D0%B7%D1%8B%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85%20(%D0%BF%D0%B0%D1%80%D1%81%D0%B8%D0%BD%D0%B3%20%D0%92%D0%B8%D0%BA%D0%B8%D0%BF%D0%B5%D0%B4%D0%B8%D0%B8).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Телеграм-бот, который будет отвечать на вопросы из вашей базы знаний.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c6WZSsChNoIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "SbG41nTn7O0R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api aiogram asyncio nest_asyncio openai tiktoken"
      ],
      "metadata": {
        "id": "ICTl6UdRPHBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716ae699-16f4-4449-dc6f-ff66798033ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: aiogram in /usr/local/lib/python3.12/dist-packages (3.22.0)\n",
            "Requirement already satisfied: asyncio in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.107.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from wikipedia-api) (2.32.4)\n",
            "Requirement already satisfied: aiofiles<24.2,>=23.2.1 in /usr/local/lib/python3.12/dist-packages (from aiogram) (24.1.0)\n",
            "Requirement already satisfied: aiohttp<3.13,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from aiogram) (3.12.15)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.12/dist-packages (from aiogram) (2025.8.3)\n",
            "Requirement already satisfied: magic-filter<1.1,>=1.0.12 in /usr/local/lib/python3.12/dist-packages (from aiogram) (1.0.12)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from aiogram) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions<=5.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from aiogram) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<3.13,>=3.9.0->aiogram) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.4.1->aiogram) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.4.1->aiogram) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.4.1->aiogram) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "import asyncio\n",
        "import logging\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.filters import Command\n",
        "import tiktoken\n",
        "import os\n",
        "import getpass\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from scipy import spatial  # вычисляет сходство векторов\n",
        "import pandas as pd\n",
        "import ast\n",
        "import re  # для вырезания ссылок <ref> из статей Википедии\n",
        "import time\n",
        "from google.colab import files\n",
        "import random\n",
        "import wikipediaapi\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import json\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "nest_asyncio.apply()\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "sJ36fUZpMlPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81401d48-0d74-435f-b9dd-07cbb6349d75"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Использование ключа API от VseGpt\n",
        "key = userdata.get('VSEGPT_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = key\n",
        "\n",
        "# Адрес сервера VseGpt\n",
        "base_url = \"https://api.vsegpt.ru/v1\"\n",
        "os.environ[\"OPENAI_BASE_URL\"] = base_url\n",
        "\n",
        "# Выбираем модель GPT\n",
        "# GPT_MODEL = \"gpt-3.5-turbo\"\n",
        "\n",
        "\n",
        "client = OpenAI(\n",
        "  api_key=key,\n",
        "  base_url=\"https://api.vsegpt.ru/v1\"\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RSETnR-qPo5-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Подготовьте данные для поиска"
      ],
      "metadata": {
        "id": "EPGHztnK34nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Замените 'ВАША_МОДЕЛЬ_ДЛЯ_ТОКЕНИЗАЦИИ' на имя модели, соответствующей вашей модели OpenAI\n",
        "# Например, для моделей gpt-3.5-turbo или gpt-4 используйте \"cl100k_base\"\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
        "    try:\n",
        "        text = text.replace(\"\\n\", \" \")\n",
        "        response = client.embeddings.create(\n",
        "            input=[text],\n",
        "            model=model\n",
        "        )\n",
        "        return response.data[0].embedding\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при получении эмбеддинга: {e}\")\n",
        "        return None\n",
        "\n",
        "def count_tokens(text: str) -> int:\n",
        "    \"\"\"Считает количество токенов в тексте с использованием токенайзера.\"\"\"\n",
        "    if not text:\n",
        "        return 0\n",
        "    return len(encoding.encode(text))"
      ],
      "metadata": {
        "id": "An3IPTkX1LFa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    language='en',\n",
        "    extract_format=wikipediaapi.ExtractFormat.WIKI,\n",
        "    user_agent='MyWikipediaBot/1.0'\n",
        ")\n",
        "\n",
        "# Конфигурация\n",
        "CATEGORY_NAME = \"Category:Psychology\"\n",
        "IGNORED_SECTIONS = [\n",
        "    \"See also\",\n",
        "    \"References\",\n",
        "    \"External links\",\n",
        "    \"Further reading\",\n",
        "    \"Footnotes\",\n",
        "    \"Bibliography\",\n",
        "    \"Sources\",\n",
        "    \"Citations\",\n",
        "    \"Literature\",\n",
        "    \"Footnotes\",\n",
        "    \"Notes and references\",\n",
        "    \"Photo gallery\",\n",
        "    \"Works cited\",\n",
        "    \"Photos\",\n",
        "    \"Gallery\",\n",
        "    \"Notes\",\n",
        "    \"References and sources\",\n",
        "    \"References and notes\",\n",
        "]\n",
        "MAX_TOKENS = 4000"
      ],
      "metadata": {
        "id": "2_OQpupp1fCD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_chunks(text, max_tokens=MAX_TOKENS):\n",
        "    def count_tokens(text):\n",
        "        return len(text) // 4\n",
        "\n",
        "    if count_tokens(text) <= max_tokens:\n",
        "        return [text]\n",
        "\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "    if len(paragraphs) > 1:\n",
        "        chunks = []\n",
        "        for paragraph in paragraphs:\n",
        "            if paragraph.strip():\n",
        "                chunks.extend(split_into_chunks(paragraph, max_tokens))\n",
        "        return chunks\n",
        "\n",
        "    sentences = sent_tokenize(text, language='english')\n",
        "    if len(sentences) > 1:\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_token_count = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_tokens = count_tokens(sentence)\n",
        "\n",
        "            if current_token_count + sentence_tokens > max_tokens and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_token_count = sentence_tokens\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_token_count += sentence_tokens\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    mid = len(text) // 2\n",
        "    split_pos = text.rfind(' ', 0, mid) or text.find(' ', mid)\n",
        "    if split_pos == -1:\n",
        "        split_pos = mid\n",
        "\n",
        "    part1 = text[:split_pos]\n",
        "    part2 = text[split_pos:]\n",
        "\n",
        "    return split_into_chunks(part1, max_tokens) + split_into_chunks(part2, max_tokens)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def process_article(title):\n",
        "    page = wiki_wiki.page(title)\n",
        "    if not page.exists():\n",
        "        return []\n",
        "\n",
        "    sections = []\n",
        "    for section in page.sections:\n",
        "        if section.title in IGNORED_SECTIONS:\n",
        "            continue\n",
        "\n",
        "        section_text = f\"Заголовок: {section.title}\\n{clean_text(section.text)}\"\n",
        "        section_chunks = split_into_chunks(section_text)\n",
        "        sections.extend(section_chunks)\n",
        "\n",
        "    return sections\n",
        "\n",
        "def get_category_members(category, depth=1):\n",
        "    if depth == 0:\n",
        "        return []\n",
        "\n",
        "    pages = []\n",
        "    for member in category.categorymembers.values():\n",
        "        if member.ns == wikipediaapi.Namespace.CATEGORY and depth > 1:\n",
        "            pages.extend(get_category_members(member, depth-1))\n",
        "        elif member.ns == wikipediaapi.Namespace.MAIN:\n",
        "            pages.append(member)\n",
        "\n",
        "    return pages"
      ],
      "metadata": {
        "id": "j0SEheln1u2e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сбор данных\n",
        "print(\"Сбор данных из Википедии...\")\n",
        "category = wiki_wiki.page(CATEGORY_NAME)\n",
        "if not category.exists():\n",
        "    print(f\"Категория {CATEGORY_NAME} не найдена!\")\n",
        "else:\n",
        "    articles = get_category_members(category)\n",
        "    knowledge_base = []\n",
        "\n",
        "    for article in articles:\n",
        "        print(f\"Обработка статьи: {article.title}\")\n",
        "        fragments = process_article(article.title)  # список строк\n",
        "\n",
        "        for fragment in fragments:\n",
        "            try:\n",
        "                embedding = get_embedding(fragment)\n",
        "                knowledge_base.append({\n",
        "                    \"text\": fragment,\n",
        "                    \"embedding\": embedding,\n",
        "                    \"source\": article.title  # опционально: откуда фрагмент\n",
        "                })\n",
        "                print(f\"  ✅ Эмбеддинг для фрагмента создан\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Ошибка при создании эмбеддинга: {e}\")\n",
        "\n",
        "    print(f\"База знаний содержит {len(knowledge_base)} фрагментов с эмбеддингами\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRG2AAzL1y2d",
        "outputId": "3499e9d1-577c-4dea-e48b-9620e59c1f4e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сбор данных из Википедии...\n",
            "Обработка статьи: Psychology\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Outline of psychology\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Alarmism\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Aphantasia\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Belief congruence\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Binocular disparity\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Binocular vision\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Biology of romantic love\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Black fatigue\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Cognitive immunization\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Counterphobic attitude\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Cybersexuality\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Double-nail illusion\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Folk economics\n",
            "Обработка статьи: Fragile masculinity\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Frequency illusion\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Impact of the COVID-19 pandemic on time perception\n",
            "Обработка статьи: Limerence\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Love addiction\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Musical escapism\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Obsessive love\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Passionate and companionate love\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Perceptual vigilance\n",
            "Обработка статьи: Psychiatry\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Psychologist\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Rapture anxiety\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Resilience week\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Social buffering\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "Обработка статьи: Ultra-realism\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "  ✅ Эмбеддинг для фрагмента создан\n",
            "База знаний содержит 125 фрагментов с эмбеддингами\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение\n",
        "with open(\"knowledge_base.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(knowledge_base, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"База знаний сохранена в knowledge_base.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-kQMqPL2KJr",
        "outputId": "84fce60d-f78b-4b69-9dec-5740391ef4b1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "База знаний сохранена в knowledge_base.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Вычисляет косинусное сходство между двумя векторами.\"\"\"\n",
        "    return np.dot(a, b) / (norm(a) * norm(b))"
      ],
      "metadata": {
        "id": "b-EOZ73Y2caE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание Telegram-бота\n",
        "API_TOKEN = userdata.get('API_TOKEN')\n",
        "bot = Bot(token=API_TOKEN)\n",
        "dp = Dispatcher()\n",
        "\n",
        "# Команда /help\n",
        "@dp.message(Command(\"help\"))\n",
        "async def send_help(message: types.Message):\n",
        "    help_text = (\n",
        "        \"Тематика: Научные статьи из Википедии\\n\"\n",
        "        f\"Число записей: {len(knowledge_base)}\\n\"\n",
        "        \"Пример запроса: 'What is Alarmism?'\"\n",
        "    )\n",
        "    await message.answer(help_text)\n",
        "\n",
        "@dp.message()\n",
        "async def handle_query(message: types.Message):\n",
        "    query = message.text\n",
        "    user_id = message.from_user.id\n",
        "\n",
        "    # 1. Считаем токены запроса\n",
        "    query_tokens = count_tokens(query)\n",
        "    if query_tokens > 4000:\n",
        "        await message.answer(\"Ваш запрос слишком длинный (более 4000 токенов).\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # 2. Получаем эмбеддинг запроса\n",
        "        query_embedding = get_embedding(query)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при получении эмбеддинга запроса: {e}\")\n",
        "        await message.answer(\"Не удалось обработать ваш запрос.\")\n",
        "        return\n",
        "\n",
        "    # 3. Считаем сходство для всех фрагментов\n",
        "    scored_chunks = []\n",
        "    for chunk in knowledge_base:\n",
        "        similarity = cosine_similarity(query_embedding, chunk[\"embedding\"])\n",
        "        text_tokens = count_tokens(chunk[\"text\"])\n",
        "        scored_chunks.append({\n",
        "            \"text\": chunk[\"text\"],\n",
        "            \"source\": chunk.get(\"source\", \"Неизвестно\"),\n",
        "            \"similarity\": similarity,\n",
        "            \"tokens\": text_tokens\n",
        "        })\n",
        "\n",
        "    # 4. Сортируем по релевантности (от самого похожего к самому непохожему)\n",
        "    scored_chunks.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
        "\n",
        "    # 5. Набираем контекст, пока не превысим 4000 токенов\n",
        "    relevant_chunks = []\n",
        "    current_tokens = query_tokens  # начинаем с токенов вопроса\n",
        "\n",
        "    # Добавим токены системного промпта — это важно!\n",
        "    system_prompt = \"You are a helpful assistant that answers questions based on the provided context.\"\n",
        "    system_tokens = count_tokens(system_prompt)\n",
        "    current_tokens += system_tokens\n",
        "\n",
        "    # Также учтём токены на оформление контекста (заголовки, разделители и т.п.)\n",
        "    # Примерно: \"Ответь на вопрос... \\n\\nВопрос: ...\" — добавим запас 50 токенов\n",
        "    overhead_tokens = 50\n",
        "    current_tokens += overhead_tokens\n",
        "\n",
        "    # Теперь добавляем фрагменты, пока не превысим лимит\n",
        "    for chunk in scored_chunks:\n",
        "        if current_tokens + chunk[\"tokens\"] <= 4000:\n",
        "            relevant_chunks.append(chunk)\n",
        "            current_tokens += chunk[\"tokens\"]\n",
        "        else:\n",
        "            break  # Прекращаем, если следующий фрагмент выведет за лимит\n",
        "\n",
        "    if not relevant_chunks:\n",
        "        await message.answer(\"Не удалось найти подходящую информацию в пределах лимита токенов.\")\n",
        "        return\n",
        "\n",
        "    # 6. Формируем контекст\n",
        "    context = (\n",
        "        \"Ответь на следующий вопрос, используя предоставленную информацию. \"\n",
        "        \"Если информация отсутствует, скажи, что не можешь ответить.\\n\\n\"\n",
        "    )\n",
        "    for chunk in relevant_chunks:\n",
        "        context += chunk[\"text\"] + \"\\n\\n\"\n",
        "    context += f\"Вопрос пользователя: {query}\"\n",
        "\n",
        "    # 7. Отправляем запрос в модель\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            # model=\"deepseek-chat-v3-0324:free\",\n",
        "            model=\"openai/gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": context}\n",
        "            ]\n",
        "        )\n",
        "        answer = response.choices[0].message.content\n",
        "\n",
        "        # Опционально: добавляем источники\n",
        "        sources = set(chunk[\"source\"] for chunk in relevant_chunks)\n",
        "        if sources:\n",
        "            answer += \"\\n\\n📚 Источники: \" + \", \".join(sources)\n",
        "\n",
        "        await message.answer(answer)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при обращении к API: {e}\")\n",
        "        await message.answer(\"Произошла ошибка при генерации ответа.\")\n"
      ],
      "metadata": {
        "id": "fQvjPHh-2flB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для запуска бота\n",
        "async def main():\n",
        "    print(\"Бот запущен\")\n",
        "    await dp.start_polling(bot)\n",
        "\n",
        "# Запуск бота\n",
        "print(\"Запуск бота...\")\n",
        "try:\n",
        "    asyncio.run(main())\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при запуске бота: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV94_yK-1LQR",
        "outputId": "c20e178b-e202-457e-ffcb-bbce8ed7744c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Запуск бота...\n",
            "Бот запущен\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:aiogram.dispatcher:Received SIGINT signal\n"
          ]
        }
      ]
    }
  ]
}